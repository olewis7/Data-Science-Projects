{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from xgboost import plot_importance, plot_tree\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "import re\n",
    "from csv import DictReader\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/olewis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(D|P)', sentence)\n",
    "    sentence = re.sub('[\\W]+', ' ', sentence.lower()) + ''.join(emoticons).replace('-', '')\n",
    "    sentence = re.sub(r'(\\d+)([a-z+]+)', r'\\1 \\2', sentence)\n",
    "    sentence = re.sub(r'([a-z+]+)(\\d+)', r'\\1 \\2', sentence)\n",
    "    slang_words = {' u ': ' you ', ' ur ': ' your ', ' ru ': ' are you ',\n",
    "                   ' r ': ' are ', ' k ': ' okay ', ' ok ': ' okay ', ' ya ': ' yes ',\n",
    "                   ' wd ': ' with ', ' hv ': ' have ', ' gv ': ' give ', ' bf ': ' boyfriend ', ' gf ': ' girlfriend ',\n",
    "                   ' lez ': ' lesbian ', ' les ': ' lesbian ', ' m ': ' male ', ' f ': ' female ',\n",
    "                   ' wanna ': ' want to ', ' gonna ': ' going to ',\n",
    "                   ' lesbo ': ' lesbian ', ' bc ': ' because ', ' plz ': ' please ', ' don t ': ' dont ',\n",
    "                   ' can t ': ' cant ', ' won t ': ' wont '}\n",
    "    for k, v in slang_words.items():\n",
    "        sentence = sentence.replace(k, v)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(name):\n",
    "    text, targets = [], []\n",
    "\n",
    "    with open('data/{}.csv'.format(name)) as f:\n",
    "        for item in DictReader(f):\n",
    "            sentence = item['text'].decode('utf8')\n",
    "            sentence = clean_sentence(sentence)\n",
    "            text.append(sentence)\n",
    "            targets.append(item['category'])\n",
    "\n",
    "    return text, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizer_porter(sentence):\n",
    "    return [porter.stem(word) for word in sentence.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_train, targets_train = read_data('train')\n",
    "text_test, targets_test = read_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(targets_train)\n",
    "encoded_Y_train = encoder.transform(targets_train)\n",
    "encoded_Y_test = encoder.transform(targets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_set = [(text_test, encoded_Y_test)]\n",
    "model = Pipeline([\n",
    "    ('vect', TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer_porter, stop_words=stop)),\n",
    "    ('clf', XGBClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_estimators = [100, 150, 200]\n",
    "max_depth = [4, 6, 8]\n",
    "learning_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "param_grid = dict(clf__max_depth=max_depth, clf__n_estimators=n_estimators, clf__learning_rate=learning_rate)\n",
    "\n",
    "kfold = StratifiedKFold(encoded_Y_train, n_folds=10, shuffle=True, random_state=7)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=\"log_loss\", n_jobs=-1, cv=kfold, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = grid_search.fit(text_train, encoded_Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting 10 folds for each of 45 candidates, totalling 450 fits\n",
    "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 47.2min\n",
    "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 273.3min\n",
    "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 631.1min\n",
    "[Parallel(n_jobs=-1)]: Done 450 out of 450 | elapsed: 646.4min finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Best parameter set: %s' % result.best_params_\n",
    "print 'CV accuracy %0.3f' % result.best_score_\n",
    "clf = result.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameter set: {'clf__max_depth': 8, 'clf__learning_rate': 0.1, 'clf__n_estimators': 200}\n",
    "CV accuracy -1.303"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = clf.predict(text_test)\n",
    "print 'macro f1:', f1_score(encoded_Y_test, predictions, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "macro f1: 0.46528675179"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [oli]",
   "language": "python",
   "name": "Python [oli]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
