{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from csv import DictReader\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import operator\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(sentence):\n",
    "    return sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizer_porter(sentence):\n",
    "    return [porter.stem(word) for word in sentence.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(D|P)', sentence)\n",
    "    sentence = re.sub('[\\W]+', ' ', sentence.lower()) + ''.join(emoticons).replace('-', '')\n",
    "    sentence = re.sub(r'(\\d+)([a-z+]+)', r'\\1 \\2', sentence)\n",
    "    sentence = re.sub(r'([a-z+]+)(\\d+)', r'\\1 \\2', sentence)\n",
    "    slang_words = {' u ': ' you ', ' ur ': ' your ', ' ru ': ' are you ', \n",
    "             ' r ': ' are ', ' k ': ' okay ', ' ok ': ' okay ', ' ya ': ' yes ', \n",
    "             ' wd ': ' with ', ' hv ': ' have ', ' gv ': ' give ', ' bf ': ' boyfriend ', ' gf ': ' girlfriend ',\n",
    "             ' lez ': ' lesbian ', ' les ': ' lesbian ', ' m ': ' male ', ' f ': ' female ',\n",
    "             ' wanna ': ' want to ', ' gonna ': ' going to ',\n",
    "             ' lesbo ': ' lesbian ', ' bc ': ' because ', ' plz ': ' please ', ' don t ': ' dont ',\n",
    "             ' can t ': ' cant ', ' won t ': ' wont '}\n",
    "    for k,v in slang_words.items():\n",
    "        sentence = sentence.replace(k, v)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(name):\n",
    "    text, targets = [], []\n",
    "\n",
    "    with open('data/{}.csv'.format(name)) as f:\n",
    "        for item in DictReader(f):\n",
    "            sentence = item['text'].decode('utf8')\n",
    "            sentence = clean_sentence(sentence)\n",
    "            text.append(sentence)\n",
    "            targets.append(item['category'])\n",
    "\n",
    "    return text, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_train, targets_train = read_data('train')\n",
    "text_test, targets_test = read_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(targets_train)\n",
    "encoded_Y_train = encoder.transform(targets_train)\n",
    "encoded_Y_test = encoder.transform(targets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                       lowercase=False,\n",
    "                       preprocessor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = [{\n",
    "        'vect__ngram_range': [(1,1)],\n",
    "        'vect__stop_words': [None, stop],\n",
    "        'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "        'clf__penalty': ['l1', 'l2'],\n",
    "        'clf__C': [1.0, 10.0, 100.0]\n",
    "            },\n",
    "             {\n",
    "        'vect__ngram_range': [(1,1), (1,2)],\n",
    "        'vect__stop_words': [stop, None],\n",
    "        'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "        'vect__use_idf': [False],\n",
    "        'vect__norm': [None],\n",
    "        'clf__penalty': ['l1', 'l2'],\n",
    "        'clf__C': [1.0, 10.0, 100.0]\n",
    "            }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_tfidf = Pipeline([('vect', tfidf), ('clf', LogisticRegression(random_state=0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, scoring='accuracy', cv=10, verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n",
    "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.3s\n",
    "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed: 10.0min\n",
    "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed: 13.3min\n",
    "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed: 22.6min finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs_lr_tfidf.fit(text_train, encoded_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Best parameter set: %s' % gs_lr_tfidf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameter set: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function tokenizer_porter at 0x7f257fc60140>, 'vect__stop_words': None, 'vect__norm': None, 'clf__penalty': 'l1', 'clf__C': 1.0, 'vect__use_idf': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'CV accuracy %0.3f' % gs_lr_tfidf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CV accuracy 0.616"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = clf.predict(text_test)\n",
    "print 'macro f1:', f1_score(encoded_Y_test, prediction, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "macro f1: 0.532216461866"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "orig_predictions = encoder.inverse_transform(prediction)\n",
    "print list(encoder.classes_)\n",
    "pd.DataFrame(confusion_matrix(targets_test, orig_predictions)).to_csv('confusion_mat.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [oli]",
   "language": "python",
   "name": "Python [oli]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
