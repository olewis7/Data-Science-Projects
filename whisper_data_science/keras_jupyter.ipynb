{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.layers import LSTM\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/olewis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('english'))\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(D|P)', sentence)\n",
    "    sentence = re.sub('[\\W]+', ' ', sentence.lower()) + ''.join(emoticons).replace('-', '')\n",
    "    sentence = re.sub(r'(\\d+)([a-z+]+)', r'n_age \\2', sentence)\n",
    "    sentence = re.sub(r'([a-z+]+)(\\d+)', r'\\1 n_age', sentence)\n",
    "    sentence = re.sub(r'(\\d+)', r'n_age', sentence)\n",
    "    slang_words = {' u ': ' you ', ' ur ': ' your ', ' ru ': ' are you ',\n",
    "                   ' r ': ' are ', ' k ': ' okay ', ' ok ': ' okay ', ' ya ': ' yes ',\n",
    "                   ' wd ': ' with ', ' hv ': ' have ', ' gv ': ' give ', ' bf ': ' boyfriend ', ' gf ': ' girlfriend ',\n",
    "                   ' lez ': ' lesbian ', ' les ': ' lesbian ', ' m ': ' male ', ' f ': ' female ',\n",
    "                   ' wanna ': ' want to ', ' gonna ': ' going to ',\n",
    "                   ' lesbo ': ' lesbian ', ' bc ': ' because ', ' plz ': ' please ', ' don t ': ' dont ',\n",
    "                   ' can t ': ' cant ', ' won t ': ' wont '}\n",
    "    for k, v in slang_words.items():\n",
    "        sentence = sentence.replace(k, v)\n",
    "\n",
    "    # Stemming words and removing stopwords:\n",
    "    sentence = \" \".join([word for word in sentence.split() if word not in stop])\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataframe_train = pd.read_csv(\"data/train.csv\")\n",
    "dataset_train = dataframe_train.values\n",
    "X_train = dataset_train[:, 1].astype(str)\n",
    "X_train = map(clean_sentence, X_train)\n",
    "Y_train = dataset_train[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe_test = pd.read_csv(\"data/test.csv\")\n",
    "dataset_test = dataframe_test.values\n",
    "X_test = dataset_test[:, 1].astype(str)\n",
    "X_test = map(clean_sentence, X_test)\n",
    "Y_test = dataset_test[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y_train)\n",
    "encoded_Y_train = encoder.transform(Y_train)\n",
    "encoded_Y_test = encoder.transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((14048, 17), (3599, 17))\n"
     ]
    }
   ],
   "source": [
    "y_train_cat = np_utils.to_categorical(encoded_Y_train)\n",
    "y_test_cat = np_utils.to_categorical(encoded_Y_test)\n",
    "print (y_train_cat.shape, y_test_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 'classes')\n"
     ]
    }
   ],
   "source": [
    "nb_classes = np.max(encoded_Y_train)+1\n",
    "print(nb_classes, 'classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_num = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_num = tokenizer.texts_to_sequences(X_test)\n",
    "X_train_mat = tokenizer.sequences_to_matrix(X_train_num)\n",
    "X_test_mat = tokenizer.sequences_to_matrix(X_test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train shape:', (14048, 10249))\n",
      "('X_test shape:', (3599, 10249))\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train_mat.shape)\n",
    "print('X_test shape:', X_test_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "nb_epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\t# create model\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_shape=(10249,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.7))   \n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "14048/14048 [==============================] - 6s - loss: 2.2236 - acc: 0.3704     \n",
      "Epoch 2/50\n",
      "14048/14048 [==============================] - 5s - loss: 1.6932 - acc: 0.4835     \n",
      "Epoch 3/50\n",
      "14048/14048 [==============================] - 5s - loss: 1.4676 - acc: 0.5398     \n",
      "Epoch 4/50\n",
      "14048/14048 [==============================] - 5s - loss: 1.3092 - acc: 0.5873     \n",
      "Epoch 5/50\n",
      "14048/14048 [==============================] - 5s - loss: 1.1800 - acc: 0.6338     \n",
      "Epoch 6/50\n",
      "14048/14048 [==============================] - 6s - loss: 1.0742 - acc: 0.6609     \n",
      "Epoch 7/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.9853 - acc: 0.6932     \n",
      "Epoch 8/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.9096 - acc: 0.7141     \n",
      "Epoch 9/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.8504 - acc: 0.7334     \n",
      "Epoch 10/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.7878 - acc: 0.7540     \n",
      "Epoch 11/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.7395 - acc: 0.7702     \n",
      "Epoch 12/50\n",
      "14048/14048 [==============================] - 7s - loss: 0.6944 - acc: 0.7833     \n",
      "Epoch 13/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.6499 - acc: 0.8006     \n",
      "Epoch 14/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.6181 - acc: 0.8093     \n",
      "Epoch 15/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.5917 - acc: 0.8163     \n",
      "Epoch 16/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.5539 - acc: 0.8299     \n",
      "Epoch 17/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.5352 - acc: 0.8312     \n",
      "Epoch 18/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.5013 - acc: 0.8420     \n",
      "Epoch 19/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.4806 - acc: 0.8531     \n",
      "Epoch 20/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.4625 - acc: 0.8577     \n",
      "Epoch 21/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.4431 - acc: 0.8618     \n",
      "Epoch 22/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.4298 - acc: 0.8699     \n",
      "Epoch 23/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.4131 - acc: 0.8739     \n",
      "Epoch 24/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.4015 - acc: 0.8726     \n",
      "Epoch 25/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.3866 - acc: 0.8785     \n",
      "Epoch 26/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.3683 - acc: 0.8873     \n",
      "Epoch 27/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.3671 - acc: 0.8860     \n",
      "Epoch 28/50\n",
      "14048/14048 [==============================] - 7s - loss: 0.3486 - acc: 0.8906     \n",
      "Epoch 29/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.3376 - acc: 0.8927     \n",
      "Epoch 30/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.3300 - acc: 0.8967     \n",
      "Epoch 31/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.3254 - acc: 0.8979     \n",
      "Epoch 32/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.3194 - acc: 0.9033     \n",
      "Epoch 33/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.3082 - acc: 0.9058     \n",
      "Epoch 34/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.2970 - acc: 0.9068     \n",
      "Epoch 35/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.2915 - acc: 0.9101     \n",
      "Epoch 36/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.2811 - acc: 0.9126     \n",
      "Epoch 37/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.2820 - acc: 0.9107     \n",
      "Epoch 38/50\n",
      "14048/14048 [==============================] - 7s - loss: 0.2729 - acc: 0.9147     \n",
      "Epoch 39/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.2593 - acc: 0.9208     \n",
      "Epoch 40/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.2616 - acc: 0.9189     \n",
      "Epoch 41/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.2555 - acc: 0.9191     \n",
      "Epoch 42/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.2509 - acc: 0.9188     \n",
      "Epoch 43/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.2460 - acc: 0.9243     \n",
      "Epoch 44/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.2391 - acc: 0.9260     \n",
      "Epoch 45/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.2344 - acc: 0.9297     \n",
      "Epoch 46/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.2288 - acc: 0.9256     \n",
      "Epoch 47/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.2228 - acc: 0.9309     \n",
      "Epoch 48/50\n",
      "14048/14048 [==============================] - 7s - loss: 0.2193 - acc: 0.9317     \n",
      "Epoch 49/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.2200 - acc: 0.9300     \n",
      "Epoch 50/50\n",
      "14048/14048 [==============================] - 6s - loss: 0.2146 - acc: 0.9329     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x184775f90>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = KerasClassifier(build_fn=baseline_model, nb_epoch=nb_epoch, batch_size=batch_size, verbose=1)\n",
    "estimator.fit(X_train_mat, y_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3599/3599 [==============================] - 0s     \n",
      "set([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16])\n",
      "['misc' 'misc' 'personal' ..., 'meetup' 'personal' 'personal']\n"
     ]
    }
   ],
   "source": [
    "predictions = estimator.predict(X_test_mat)\n",
    "print(set(predictions))\n",
    "print(encoder.inverse_transform(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1: 0.486282499478\n"
     ]
    }
   ],
   "source": [
    "print 'macro f1:', f1_score(encoded_Y_test, predictions, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
